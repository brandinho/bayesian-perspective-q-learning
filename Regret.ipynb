{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regret.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/opc3047IRpAFdy0L9hN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandinho/Baysian-Q-Learning/blob/master/Regret.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Gx2Kq4Gz4U",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation of Q-Learning Algorithms\n",
        "\n",
        "We evaluate various Q-Learning algirithms by comparing their cumulative regret as training progresses. A lower and flatter curve indicates that the agent is better at solving the exploration vs exploitation problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeeSPK7aLR64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from math import erf\n",
        "from scipy.special import ndtri\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAJgYE3EHZrg",
        "colab_type": "text"
      },
      "source": [
        "First, we define our Q-Learning agent, which we call \"TabularAgent\" because we apply it to a tabular setting (Grid World). The agent is flexible enough to accomodate various Q-Learning approaches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0oy2L8yfy7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TabularAgent():\n",
        "  def __init__(self, grid_template, starting_location, possible_actions, epsilon_greedy, epsilon_decay, \n",
        "               epsilon_decay_rate = 0.001, epsilon = 0.1, gamma = 0.99, alpha = 0.05, is_bayesian = True, \n",
        "               upper_confidence_bound = False, myopic_vpi = True):\n",
        "    self.position = starting_location\n",
        "    self.possible_actions = possible_actions\n",
        "    self.n_actions = len(self.possible_actions)\n",
        "    \n",
        "    starting_mean = 0\n",
        "    starting_var = 50\n",
        "    \n",
        "    height = grid_template.shape[0]\n",
        "    width = grid_template.shape[1]\n",
        "\n",
        "    self.Q_table_mean = {}\n",
        "    self.Q_table_var = {}\n",
        "    self.reward_variance_tracker = {}\n",
        "    for x in range(width):\n",
        "      for y in range(height):\n",
        "        for action in self.possible_actions:\n",
        "          current_key = self.key_format((y, x), action)\n",
        "          self.Q_table_mean[current_key] = starting_mean\n",
        "          self.Q_table_var[current_key] = starting_var\n",
        "          self.reward_variance_tracker[current_key] = {\"variance\": 0, \"mean\": None, \"SSE\": 0, \"idx\": 0}\n",
        "\n",
        "    self.epsilon_greedy = epsilon_greedy\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_decay_rate = epsilon_decay_rate\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = gamma\n",
        "    self.alpha = alpha\n",
        "    \n",
        "    self.is_bayesian = is_bayesian\n",
        "    self.upper_confidence_bound = upper_confidence_bound\n",
        "    self.myopic_vpi = myopic_vpi\n",
        "    self.episode = 0\n",
        "        \n",
        "  @staticmethod\n",
        "  def key_format(position, action):\n",
        "    return \"{} {}\".format(position, action)   \n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "  @staticmethod\n",
        "  def phi(x, mu, sigma):\n",
        "      \"Much faster computation compared to scipy.stats norm.cdf\"\n",
        "      return (1 + erf((x - mu) / sigma / np.sqrt(2))) / 2\n",
        "  \n",
        "  @staticmethod\n",
        "  def phi_inverse(alpha):\n",
        "      return ndtri(alpha)\n",
        "  \n",
        "  @staticmethod\n",
        "  def lower_phi(y):\n",
        "      return (1 / np.sqrt(2 * np.pi)) * np.exp(-y**2/2)\n",
        "  \n",
        "  def expected_value(self, mu, sigma, upper_bound = None, lower_bound = None):\n",
        "      \"Much faster computation compared to scipy.stats norm.expect\"\n",
        "      assert upper_bound is not None or lower_bound is not None\n",
        "      if upper_bound is not None:\n",
        "          alpha = 1 - self.phi(upper_bound, mu, sigma)\n",
        "          direction = -1\n",
        "      elif lower_bound is not None:\n",
        "          alpha = self.phi(lower_bound, mu, sigma)\n",
        "          direction = 1\n",
        "          \n",
        "      if alpha == 1:\n",
        "          return 0\n",
        "      elif alpha == 0:\n",
        "          return mu\n",
        "      else:\n",
        "          return (mu + direction * sigma * ((self.lower_phi(self.phi_inverse(alpha))) / (1 - alpha))) * (1 - alpha)        \n",
        "\n",
        "  def get_myopic_vpi(self, a, best_action, second_best_action):\n",
        "      current_mu = self.Q_table_mean[self.key_format(self.position, a)]\n",
        "      current_sigma = np.sqrt(self.Q_table_var[self.key_format(self.position, a)])\n",
        "      if a == best_action:\n",
        "          second_best_mu = self.Q_table_mean[self.key_format(self.position, second_best_action)]\n",
        "          probability_of_gain = self.phi(second_best_mu, current_mu, current_sigma)\n",
        "          expected_better_value = self.expected_value(current_mu, current_sigma, upper_bound = second_best_mu)\n",
        "          vpi = second_best_mu * probability_of_gain - expected_better_value\n",
        "      else:\n",
        "          best_mu = self.Q_table_mean[self.key_format(self.position, best_action)]\n",
        "          probability_of_gain = 1 - self.phi(best_mu, current_mu, current_sigma)\n",
        "          expected_better_value = self.expected_value(current_mu, current_sigma, lower_bound = best_mu)\n",
        "          vpi = expected_better_value - best_mu * probability_of_gain\n",
        "      return vpi\n",
        "\n",
        "  def update_reward_variance(self, state, action, reward):\n",
        "    key = self.key_format(state, action)\n",
        "    self.reward_variance_tracker[key][\"idx\"] += 1\n",
        "    idx = self.reward_variance_tracker[key][\"idx\"]\n",
        "    if self.reward_variance_tracker[key][\"mean\"] is None:\n",
        "      self.reward_variance_tracker[key][\"mean\"] = reward\n",
        "    error = reward - self.reward_variance_tracker[key][\"mean\"]\n",
        "    \n",
        "    self.reward_variance_tracker[key][\"mean\"] += error / (idx + 1)\n",
        "    self.reward_variance_tracker[key][\"SSE\"] += error * (reward - self.reward_variance_tracker[key][\"mean\"])\n",
        "    self.reward_variance_tracker[key][\"variance\"] = self.reward_variance_tracker[key][\"SSE\"] / (idx + 1)\n",
        "\n",
        "  def sample_Q(self, key):\n",
        "    return np.random.normal(loc = self.Q_table_mean[key], scale = np.sqrt(self.Q_table_var[key]))\n",
        "\n",
        "  def get_action(self, stochastic = True):\n",
        "    action = np.zeros(self.n_actions)\n",
        "    if stochastic and self.is_bayesian:\n",
        "      if self.upper_confidence_bound:\n",
        "        Qvalue_list = [self.Q_table_mean[self.key_format(self.position, a)] + np.sqrt(self.Q_table_var[self.key_format(self.position, a)]) * 2 for a in self.possible_actions]\n",
        "      elif self.myopic_vpi:\n",
        "        Q_means = [self.Q_table_mean[self.key_format(self.position, a)]for a in self.possible_actions]\n",
        "        best_action = self.possible_actions[np.argmax(Q_means)]\n",
        "        second_best_action = self.possible_actions[np.argsort(Q_means)[-2]]\n",
        "        Qvalue_list = [self.Q_table_mean[self.key_format(self.position, a)] + self.get_myopic_vpi(a, best_action, second_best_action) for a in self.possible_actions]        \n",
        "      else:\n",
        "        Qvalue_list = [self.sample_Q(self.key_format(self.position, a)) for a in self.possible_actions]\n",
        "    elif stochastic and self.epsilon_greedy and np.random.rand() < self.epsilon:\n",
        "      Qvalue_list = np.random.normal(size = len(self.possible_actions))\n",
        "    else:\n",
        "      Qvalue_list = [self.Q_table_mean[self.key_format(self.position, a)] for a in self.possible_actions]\n",
        "    action[np.argmax(Qvalue_list)] = 1\n",
        "    return action\n",
        "  \n",
        "  def change_position(self, current_position, direction):\n",
        "    if direction == \"Left\":\n",
        "      current_position = (current_position[0], current_position[1] - 1)\n",
        "    elif direction == \"Right\":\n",
        "      current_position = (current_position[0], current_position[1] + 1)\n",
        "    elif direction == \"Up\":\n",
        "      current_position = (current_position[0] - 1, current_position[1])\n",
        "    elif direction == \"Down\":\n",
        "      current_position = (current_position[0] + 1, current_position[1])\n",
        "    return current_position\n",
        "\n",
        "  def move(self, action):\n",
        "    self.position = self.change_position(self.position, action)\n",
        "    return self.position        \n",
        "      \n",
        "  def learn(self, state, action, reward, next_state):\n",
        "    # Lookahead Mean\n",
        "    old_Q_value_mean = self.Q_table_mean[self.key_format(state, action)]\n",
        "    next_action = self.possible_actions[np.argmax([self.Q_table_mean[self.key_format(next_state, a)] for a in self.possible_actions])]\n",
        "    next_Q_value_mean = self.Q_table_mean[self.key_format(next_state, next_action)]\n",
        "    lookahead_Q_value_mean = reward + self.gamma * next_Q_value_mean\n",
        "    \n",
        "    if self.is_bayesian:\n",
        "      # Lookahead Variance\n",
        "      self.update_reward_variance(state, action, reward)\n",
        "      reward_var = self.reward_variance_tracker[self.key_format(state, action)][\"variance\"]\n",
        "      old_Q_value_var = self.Q_table_var[self.key_format(state, action)] + 0.001 # The constant prevents the agent's uncertainty from going to 0 (philosophically we don't believe it should ever reach 0)\n",
        "      next_Q_value_var = self.Q_table_var[self.key_format(next_state, next_action)]\n",
        "      lookahead_Q_value_var = reward_var + self.gamma * next_Q_value_var            \n",
        "    \n",
        "    # Update Mean and Variance\n",
        "    if self.is_bayesian:\n",
        "      alpha = old_Q_value_var / (old_Q_value_var + lookahead_Q_value_var)\n",
        "    else:\n",
        "      alpha = self.alpha\n",
        "    Q_key = self.key_format(state, action)\n",
        "    self.Q_table_mean[Q_key] = (1 - alpha) * old_Q_value_mean + alpha * lookahead_Q_value_mean\n",
        "    if self.is_bayesian:\n",
        "      self.Q_table_var[Q_key] = alpha * lookahead_Q_value_var\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUM0O05DHrvN",
        "colab_type": "text"
      },
      "source": [
        "Next, we want to create the environment for the agent to navigate through. If you are feeling adventurous, you can change the location of the danger zone and the parameters of the reward distributions! We initially named the danger zone the \"random forest\" because the rewards that the agent received were largely unknown... and also we thought it was a clever play on words... But in the paper, we ultimately decided to call it the danger zone to avoid confusion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDaIqouIfii7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment():\n",
        "  def __init__(self, agent_inputs, height, width):        \n",
        "    assert width > 2 and height > 2        \n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    self.grid_template = np.zeros((self.height, self.width))\n",
        "    self.state_visitations = self.grid_template.copy()\n",
        "    self.danger_probability = 0.1\n",
        "    \n",
        "    ### Define the area for the danger zone\n",
        "    self.height_anchor = 1\n",
        "    self.height_offset = 3\n",
        "    self.width_anchor = 1\n",
        "    self.width_offset = 2\n",
        "    \n",
        "    self.dangerY_range = range(self.height_anchor, self.height_anchor + self.height_offset)\n",
        "    self.dangerX_range = range(self.width_anchor, self.width_anchor + self.width_offset)\n",
        "    self.dangerY_len, self.dangerX_len = len(self.dangerY_range), len(self.dangerX_range)\n",
        "    \n",
        "    ### Define some important locations\n",
        "    self.starting_location = (0, 0)\n",
        "    self.goal_location = (0, self.width-1)\n",
        "    self.cliff_locations = np.array([(0, cliff) for cliff in range(1, self.width-1)])\n",
        "    \n",
        "    ### Define some rewards\n",
        "    self.goal_reward = 50\n",
        "    self.cliff_reward = -50\n",
        "    self.roaming_loc, self.roaming_scale = -2, 2\n",
        "    self.danger_loc, self.danger_scale = -15, 15\n",
        "    self.regret_baseline = self.roaming_loc * 16 + self.goal_reward\n",
        "    \n",
        "    ### Addign the rewards to the grid\n",
        "    self.set_default_grid_rewards()\n",
        "    self.cumulative_rewards = 0\n",
        "    self.cumulative_expected_rewards = 0\n",
        "    self.n_moves = 0\n",
        "    self.max_moves = self.height * self.width\n",
        "\n",
        "    self.vertical_min, self.vertical_max = 0, self.height-1\n",
        "    self.horizontal_min, self.horizontal_max = 0, self.width-1\n",
        "    self.possible_actions = np.array([\"Left\", \"Right\", \"Up\", \"Down\"])\n",
        "\n",
        "    self.agent = TabularAgent(self.grid_template, self.starting_location, self.possible_actions, \n",
        "                              agent_inputs[\"Epsilon-Greedy\"], agent_inputs[\"Epsilon Decay\"], \n",
        "                              agent_inputs[\"Epsilon Decay Rate\"], agent_inputs[\"Epsilon\"], \n",
        "                              agent_inputs[\"Gamma\"], agent_inputs[\"Alpha\"], agent_inputs[\"Bayesian\"], \n",
        "                              agent_inputs[\"Upper Confidence Bound\"], agent_inputs[\"Myopic-VPI\"])\n",
        "    self.nullify_gameover_Q_values()\n",
        "      \n",
        "  def nullify_gameover_Q_values(self):\n",
        "    for a in self.possible_actions:\n",
        "      key = self.agent.key_format(self.goal_location, a)\n",
        "      self.agent.Q_table_mean[key] = 0\n",
        "      self.agent.Q_table_var[key] = 1\n",
        "\n",
        "  def set_default_grid_rewards(self):\n",
        "    self.grid_rewards = self.grid_template.copy()\n",
        "    self.grid_rewards[:] = np.random.normal(self.roaming_loc, self.roaming_scale, size = self.height * self.width).reshape(self.height, self.width)\n",
        "    self.grid_rewards[self.goal_location] = self.goal_reward\n",
        "    self.grid_rewards[tuple(self.cliff_locations.T)] = self.cliff_reward       \n",
        "      \n",
        "  def set_random_cliffs(self):\n",
        "    danger_conditional = np.ones_like(self.grid_template)        \n",
        "    random_numbers = np.random.rand(self.dangerY_len * self.dangerX_len).reshape(self.dangerY_len, self.dangerX_len)\n",
        "    danger_conditional[self.dangerY_range[0]:self.dangerY_range[-1]+1, self.dangerX_range[0]:self.dangerX_range[-1]+1] = random_numbers\n",
        "    self.grid_rewards[danger_conditional < self.danger_probability] = self.cliff_reward\n",
        "\n",
        "  def set_random_forest_rewards(self):\n",
        "    random_rewards = np.random.normal(loc = self.danger_loc, scale = self.danger_scale, size = self.height_offset * self.width_offset)\n",
        "    random_rewards = random_rewards.reshape(self.height_offset, self.width_offset)\n",
        "    self.grid_rewards[self.dangerY_range[0]:self.dangerY_range[-1]+1, \n",
        "                      self.dangerX_range[0]:self.dangerX_range[-1]+1] = random_rewards\n",
        "\n",
        "  @staticmethod\n",
        "  def onehot_to_index(onehot):\n",
        "    return np.where(onehot==1)[0][0]  \n",
        "  \n",
        "  def check_valid_action(self, action_onehot):\n",
        "    assert len(action_onehot) == len(self.possible_actions)\n",
        "    action = self.possible_actions[self.onehot_to_index(action_onehot)]\n",
        "    valid_action = True\n",
        "    if action == \"Left\":\n",
        "      if self.agent.position[1] == 0:\n",
        "        valid_action = False\n",
        "    elif action == \"Right\":\n",
        "      if self.agent.position[1] == self.width - 1:\n",
        "        valid_action = False\n",
        "    elif action == \"Up\":\n",
        "      if self.agent.position[0] == 0:\n",
        "        valid_action = False\n",
        "    elif action == \"Down\":\n",
        "      if self.agent.position[0] == self.height - 1:\n",
        "        valid_action = False\n",
        "    return valid_action, action\n",
        "\n",
        "  def move(self, stochastic = True, verbose = False, learn = True):\n",
        "    state = self.agent.position\n",
        "    self.state_visitations[state[0], state[1]] += 1\n",
        "    action_onehot = self.agent.get_action(stochastic)\n",
        "    valid_action, action = self.check_valid_action(action_onehot)\n",
        "    if valid_action:            \n",
        "      next_state = self.agent.move(action)\n",
        "    else:\n",
        "      next_state = self.agent.position\n",
        "    self.n_moves += 1\n",
        "    \n",
        "    ### Reset the rewards/punishments in the random forest\n",
        "    self.set_default_grid_rewards()\n",
        "    self.set_random_forest_rewards()\n",
        "        \n",
        "    reward = self.grid_rewards[next_state]\n",
        "    self.cumulative_rewards += reward\n",
        "    if next_state == self.goal_location:\n",
        "      self.cumulative_expected_rewards += self.goal_reward\n",
        "    elif next_state in self.cliff_locations.tolist():\n",
        "      self.cumulative_expected_rewards += self.cliff_reward\n",
        "    elif (next_state[0] >= self.dangerY_range[0] and next_state[0] < self.dangerY_range[-1] and\n",
        "          next_state[1] >= self.dangerX_range[0] and next_state[1] < self.dangerX_range[-1]):\n",
        "      self.cumulative_expected_rewards += self.danger_loc\n",
        "    else:\n",
        "      self.cumulative_expected_rewards += self.roaming_loc\n",
        "    \n",
        "    if learn:\n",
        "      self.agent.learn(state, action, reward, next_state)  \n",
        "\n",
        "    done_game = False\n",
        "    if self.agent.position == self.goal_location:\n",
        "      game_end_text = \"You won the game!! Congratz!\"\n",
        "      done_game = True\n",
        "    if any([self.agent.position == tuple(cliff) for cliff in self.cliff_locations]):\n",
        "      game_end_text = \"You fell off the cliff!\"\n",
        "      done_game = True\n",
        "    if self.n_moves >= self.max_moves:\n",
        "      game_end_text = \"You timed out!\"\n",
        "      done_game = True\n",
        "        \n",
        "    if done_game:\n",
        "      self.state_visitations[next_state[0], next_state[1]] += 1\n",
        "      if self.agent.epsilon_decay:\n",
        "        self.agent.epsilon = max(0.02, self.agent.epsilon - self.agent.epsilon_decay_rate)\n",
        "      if verbose:\n",
        "        print(game_end_text)\n",
        "    return done_game        \n",
        "  \n",
        "  def reset(self):\n",
        "    self.agent.position = self.starting_location\n",
        "    self.cumulative_rewards = 0\n",
        "    self.cumulative_expected_rewards = 0\n",
        "    self.n_moves = 0\n",
        "    self.agent.episode += 1\n",
        "\n",
        "  def update_environment_visualization(self, ax):\n",
        "    self.env_map.set_data(self.grid_rewards)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP6YT_q0In3V",
        "colab_type": "text"
      },
      "source": [
        "Below is the training loop that records cumulative regret"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmbXaEusn0g2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_regret_from_training(env, episodes, label):\n",
        "  total_returns = []\n",
        "  cumulative_regret = []\n",
        "  for e in tqdm(range(episodes)):\n",
        "    env.reset()\n",
        "    done_game = False\n",
        "    while not done_game:\n",
        "      done_game = env.move()\n",
        "    total_returns.append(env.cumulative_rewards)\n",
        "    cumulative_regret.append(env.regret_baseline - env.cumulative_expected_rewards)\n",
        "  plt.plot(np.cumsum(cumulative_regret), label = label)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-h0Q89Nnwcw",
        "colab_type": "text"
      },
      "source": [
        "Define a function to get model parameters given the chosen model (mostly to clean up the code in the last section):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbp2nDQ9n1xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_parameters(model_name):\n",
        "  if \"Epsilon\" not in model_name:\n",
        "    is_bayesian = True\n",
        "    epsilon_greedy = False\n",
        "    epsilon_decay = False\n",
        "  else:\n",
        "    is_bayesian = False\n",
        "    epsilon_greedy = True\n",
        "    if \"Decay\" in model_name:\n",
        "      epsilon_decay = True\n",
        "    else:\n",
        "      epsilon_decay = False\n",
        "\n",
        "  if \"UCB\" in model_name:\n",
        "    upper_confidence_bound = True\n",
        "  else:\n",
        "    upper_confidence_bound = False\n",
        "\n",
        "  if \"Myopic-VPI\" in model_name:\n",
        "    myopic_vpi = True\n",
        "  else:\n",
        "    myopic_vpi = False\n",
        "\n",
        "  return epsilon_greedy, epsilon_decay, is_bayesian, upper_confidence_bound, myopic_vpi"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-MNsowFIufZ",
        "colab_type": "text"
      },
      "source": [
        "Define the parameters of the benchmark that you want to compare our approach against"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZek6vROKRjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark_agent_inputs = {\n",
        "            \"Bayesian\": False, \n",
        "            \"Upper Confidence Bound\": False,  \n",
        "            \"Myopic-VPI\": False,\n",
        "            \"Epsilon-Greedy\": False, \n",
        "            \"Epsilon Decay\": False,\n",
        "            \"Epsilon\": 0.1, \n",
        "            \"Epsilon Decay Rate\": 0.0001,\n",
        "            \"Gamma\": 0.99, \n",
        "            \"Alpha\": 0.05\n",
        "        }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLO5eSFI0LD",
        "colab_type": "text"
      },
      "source": [
        "Now, we're ready to play around with some parameters and see how our agent performs relative to any benchmark!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J62zyNvCm6Np",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "b4187c00-4eb8-47c9-f133-f1bd71f225a2"
      },
      "source": [
        "#@title Play Around with the Parameters { form-width: \"400px\" }\n",
        "\n",
        "model_name = \"Myopic-VPI\" #@param [\"Epsilon-Greedy\", \"Epsilon-Greedy (Decay)\", \"Bayes-UCB\", \"Q-Value Sampling\", \"Myopic-VPI\"] {type:\"string\"}\n",
        "\n",
        "epsilon_greedy, epsilon_decay, is_bayesian, upper_confidence_bound, myopic_vpi = get_model_parameters(model_name)\n",
        "\n",
        "epsilon = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "epsilon_decay_rate = 0.01 #@param {type:\"slider\", min:0.0001, max:0.01, step:0.0001}\n",
        "gamma = 0.99 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "alpha = 0.05 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "agent_inputs = {\n",
        "            \"Bayesian\": is_bayesian, \"Upper Confidence Bound\": upper_confidence_bound, \"Myopic-VPI\": myopic_vpi,\n",
        "            \"Epsilon-Greedy\": epsilon_greedy, \"Epsilon Decay\": epsilon_decay, \"Epsilon\": epsilon, \n",
        "            \"Epsilon Decay Rate\": epsilon_decay_rate, \"Gamma\": gamma, \"Alpha\": alpha\n",
        "        }\n",
        "\n",
        "env = Environment(benchmark_agent_inputs, height = 6, width = 10)\n",
        "get_regret_from_training(env, episodes = 5000, label = \"Benchmark\")\n",
        "\n",
        "env = Environment(agent_inputs, height = 6, width = 10)\n",
        "get_regret_from_training(env, episodes = 5000, label = \"Your Choice\")\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:09<00:00, 532.46it/s]\n",
            "100%|██████████| 5000/5000 [00:18<00:00, 272.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8ff8c79048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}